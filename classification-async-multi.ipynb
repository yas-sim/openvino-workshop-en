{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technique for high performance inference program - asynchronous and simultaneous inferencing\n",
    "You will learn the basic technique to develop an efficient and high performance OpenVINO application using asynchronous and simultaneous inferencing.   \n",
    "We'll continue to use a simple image classification program as an example to make things as simple as possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check hardware configration of your system (DevCloud development server, in this case)\n",
    "Before start, let's check how many CPU cores does the system has for the optimization work later.  \n",
    "Install `psutil` Python module to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linux\n",
    "!pip3 install psutil\n",
    "import psutil\n",
    "print('# of CPU cores = {}C/{}T'.format(psutil.cpu_count(logical=False), psutil.cpu_count(logical=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psutil\n",
      "  Downloading psutil-5.7.0-cp36-cp36m-win_amd64.whl (235 kB)\n",
      "Installing collected packages: psutil\n",
      "Successfully installed psutil-5.7.0\n",
      "# of CPU cores = 4C/8T\n"
     ]
    }
   ],
   "source": [
    "# Windows\n",
    "!pip install psutil\n",
    "import psutil\n",
    "print('# of CPU cores = {}C/{}T'.format(psutil.cpu_count(logical=False), psutil.cpu_count(logical=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing an input image and label text data files\n",
    "Next, let's prepare imput image file and class label text file. Those files are in the OpenVINO install directory. We'll simply copy them to the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linux\n",
    "!cp $INTEL_OPENVINO_DIR/deployment_tools/demo/car.png .\n",
    "!cp $INTEL_OPENVINO_DIR/deployment_tools/demo/squeezenet1.1.labels synset_words.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        1 個のファイルをコピーしました。\n",
      "        1 個のファイルをコピーしました。\n"
     ]
    }
   ],
   "source": [
    "# Windows\n",
    "!copy \"%INTEL_OPENVINO_DIR%\\deployment_tools\\demo\\car.png\" .\n",
    "!copy \"%INTEL_OPENVINO_DIR%\\deployment_tools\\demo\\squeezenet1.1.labels\" synset_words.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing a DL model for inferencing\n",
    "Download a DL model for image classification using `Model downloader` and convert it into OpenVINO IR model with `Model converter`.  \n",
    "We'll use `googlenet-v1` model for this practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linux\n",
    "!python3 $INTEL_OPENVINO_DIR/deployment_tools/tools/model_downloader/downloader.py --name googlenet-v1\n",
    "!python3 $INTEL_OPENVINO_DIR/deployment_tools/tools/model_downloader/converter.py  --name googlenet-v1 --precisions FP16\n",
    "!ls public/googlenet-v1/FP16 -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################|| Downloading models ||################\n",
      "\n",
      "========== Downloading C:\\Users\\yas_s\\Documents\\Intel\\OpenVINO\\openvino-workshop-en\\public\\googlenet-v1\\googlenet-v1.prototxt\n",
      "\n",
      "========== Downloading C:\\Users\\yas_s\\Documents\\Intel\\OpenVINO\\openvino-workshop-en\\public\\googlenet-v1\\googlenet-v1.caffemodel\n",
      "... 1%, 1024 KB, 1284 KB/s, 0 seconds passed\n",
      "... 3%, 2048 KB, 1986 KB/s, 1 seconds passed\n",
      "... 5%, 3072 KB, 2457 KB/s, 1 seconds passed\n",
      "... 7%, 4096 KB, 2788 KB/s, 1 seconds passed\n",
      "... 9%, 5120 KB, 3033 KB/s, 1 seconds passed\n",
      "... 11%, 6144 KB, 3303 KB/s, 1 seconds passed\n",
      "... 13%, 7168 KB, 3555 KB/s, 2 seconds passed\n",
      "... 15%, 8192 KB, 3799 KB/s, 2 seconds passed\n",
      "... 17%, 9216 KB, 3984 KB/s, 2 seconds passed\n",
      "... 19%, 10240 KB, 4174 KB/s, 2 seconds passed\n",
      "... 21%, 11264 KB, 4291 KB/s, 2 seconds passed\n",
      "... 23%, 12288 KB, 4442 KB/s, 2 seconds passed\n",
      "... 25%, 13312 KB, 4604 KB/s, 2 seconds passed\n",
      "... 27%, 14336 KB, 4729 KB/s, 3 seconds passed\n",
      "... 29%, 15360 KB, 4866 KB/s, 3 seconds passed\n",
      "... 31%, 16384 KB, 4969 KB/s, 3 seconds passed\n",
      "... 33%, 17408 KB, 5087 KB/s, 3 seconds passed\n",
      "... 35%, 18432 KB, 5173 KB/s, 3 seconds passed\n",
      "... 37%, 19456 KB, 5254 KB/s, 3 seconds passed\n",
      "... 39%, 20480 KB, 5327 KB/s, 3 seconds passed\n",
      "... 41%, 21504 KB, 5396 KB/s, 3 seconds passed\n",
      "... 43%, 22528 KB, 5461 KB/s, 4 seconds passed\n",
      "... 45%, 23552 KB, 5561 KB/s, 4 seconds passed\n",
      "... 47%, 24576 KB, 5636 KB/s, 4 seconds passed\n",
      "... 48%, 25600 KB, 5707 KB/s, 4 seconds passed\n",
      "... 50%, 26624 KB, 5795 KB/s, 4 seconds passed\n",
      "... 52%, 27648 KB, 5858 KB/s, 4 seconds passed\n",
      "... 54%, 28672 KB, 5919 KB/s, 4 seconds passed\n",
      "... 56%, 29696 KB, 5995 KB/s, 4 seconds passed\n",
      "... 58%, 30720 KB, 6049 KB/s, 5 seconds passed\n",
      "... 60%, 31744 KB, 6118 KB/s, 5 seconds passed\n",
      "... 62%, 32768 KB, 6167 KB/s, 5 seconds passed\n",
      "... 64%, 33792 KB, 6232 KB/s, 5 seconds passed\n",
      "... 66%, 34816 KB, 6276 KB/s, 5 seconds passed\n",
      "... 68%, 35840 KB, 6300 KB/s, 5 seconds passed\n",
      "... 70%, 36864 KB, 6359 KB/s, 5 seconds passed\n",
      "... 72%, 37888 KB, 6397 KB/s, 5 seconds passed\n",
      "... 74%, 38912 KB, 6451 KB/s, 6 seconds passed\n",
      "... 76%, 39936 KB, 6503 KB/s, 6 seconds passed\n",
      "... 78%, 40960 KB, 6553 KB/s, 6 seconds passed\n",
      "... 80%, 41984 KB, 6601 KB/s, 6 seconds passed\n",
      "... 82%, 43008 KB, 6648 KB/s, 6 seconds passed\n",
      "... 84%, 44032 KB, 6677 KB/s, 6 seconds passed\n",
      "... 86%, 45056 KB, 6721 KB/s, 6 seconds passed\n",
      "... 88%, 46080 KB, 6748 KB/s, 6 seconds passed\n",
      "... 90%, 47104 KB, 6774 KB/s, 6 seconds passed\n",
      "... 92%, 48128 KB, 6814 KB/s, 7 seconds passed\n",
      "... 94%, 49152 KB, 6853 KB/s, 7 seconds passed\n",
      "... 95%, 50176 KB, 6876 KB/s, 7 seconds passed\n",
      "... 97%, 51200 KB, 6898 KB/s, 7 seconds passed\n",
      "... 99%, 52224 KB, 6919 KB/s, 7 seconds passed\n",
      "... 100%, 52279 KB, 6927 KB/s, 7 seconds passed\n",
      "\n",
      "################|| Post-processing ||################\n",
      "\n",
      "========== Replacing text in C:\\Users\\yas_s\\Documents\\Intel\\OpenVINO\\openvino-workshop-en\\public\\googlenet-v1\\googlenet-v1.prototxt\n",
      "========= Converting googlenet-v1 to IR (FP16)\n",
      "Conversion command: C:\\Users\\yas_s\\AppData\\Local\\Programs\\Python\\Python36\\python.exe -- \"C:\\Program Files (x86)\\IntelSWTools\\openvino\\deployment_tools\\model_optimizer\\mo.py\" --framework=caffe --data_type=FP16 --output_dir=C:\\Users\\yas_s\\Documents\\Intel\\OpenVINO\\openvino-workshop-en\\public\\googlenet-v1\\FP16 --model_name=googlenet-v1 --input_shape=[1,3,224,224] --input=data --mean_values=data[104.0,117.0,123.0] --output=prob --input_model=C:\\Users\\yas_s\\Documents\\Intel\\OpenVINO\\openvino-workshop-en\\public\\googlenet-v1/googlenet-v1.caffemodel --input_proto=C:\\Users\\yas_s\\Documents\\Intel\\OpenVINO\\openvino-workshop-en\\public\\googlenet-v1/googlenet-v1.prototxt\n",
      "\n",
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \tC:\\Users\\yas_s\\Documents\\Intel\\OpenVINO\\openvino-workshop-en\\public\\googlenet-v1/googlenet-v1.caffemodel\n",
      "\t- Path for generated IR: \tC:\\Users\\yas_s\\Documents\\Intel\\OpenVINO\\openvino-workshop-en\\public\\googlenet-v1\\FP16\n",
      "\t- IR output name: \tgooglenet-v1\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tdata\n",
      "\t- Output layers: \tprob\n",
      "\t- Input shapes: \t[1,3,224,224]\n",
      "\t- Mean values: \tdata[104.0,117.0,123.0]\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP16\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- Enable grouped convolutions fusing: \tTrue\n",
      "\t- Move mean values to preprocess section: \tFalse\n",
      "\t- Reverse input channels: \tFalse\n",
      "Caffe specific parameters:\n",
      "\t- Path to Python Caffe* parser generated from caffe.proto: \tC:\\Program Files (x86)\\IntelSWTools\\openvino\\deployment_tools\\model_optimizer\\mo\\front\\caffe\\proto\n",
      "\t- Enable resnet optimization: \tTrue\n",
      "\t- Path to the Input prototxt: \tC:\\Users\\yas_s\\Documents\\Intel\\OpenVINO\\openvino-workshop-en\\public\\googlenet-v1/googlenet-v1.prototxt\n",
      "\t- Path to CustomLayersMapping.xml: \tC:\\Program Files (x86)\\IntelSWTools\\openvino\\deployment_tools\\model_optimizer\\extensions\\front\\caffe\\CustomLayersMapping.xml\n",
      "\t- Path to a mean file: \tNot specified\n",
      "\t- Offsets for a mean file: \tNot specified\n",
      "Model Optimizer version: \t2020.1.0-61-gd349c3ba4a\n",
      "\n",
      "[ SUCCESS ] Generated IR version 10 model.\n",
      "[ SUCCESS ] XML file: C:\\Users\\yas_s\\Documents\\Intel\\OpenVINO\\openvino-workshop-en\\public\\googlenet-v1\\FP16\\googlenet-v1.xml\n",
      "[ SUCCESS ] BIN file: C:\\Users\\yas_s\\Documents\\Intel\\OpenVINO\\openvino-workshop-en\\public\\googlenet-v1\\FP16\\googlenet-v1.bin\n",
      "[ SUCCESS ] Total execution time: 10.01 seconds. \n",
      "\n",
      " ドライブ C のボリューム ラベルは Windows です\n",
      " ボリューム シリアル番号は 2A13-3B60 です\n",
      "\n",
      " C:\\Users\\yas_s\\Documents\\Intel\\OpenVINO\\openvino-workshop-en\\public\\googlenet-v1\\FP16 のディレクトリ\n",
      "\n",
      "2020/04/13  11:23    <DIR>          .\n",
      "2020/04/13  11:23    <DIR>          ..\n",
      "2020/04/13  11:37        13,997,134 googlenet-v1.bin\n",
      "2020/04/13  11:37            22,420 googlenet-v1.mapping\n",
      "2020/04/13  11:37           160,850 googlenet-v1.xml\n",
      "               3 個のファイル          14,180,404 バイト\n",
      "               2 個のディレクトリ  359,048,683,520 バイトの空き領域\n"
     ]
    }
   ],
   "source": [
    "# Windows\n",
    "!python \"%INTEL_OPENVINO_DIR%\\deployment_tools\\tools\\model_downloader\\downloader.py\" --name googlenet-v1\n",
    "!python \"%INTEL_OPENVINO_DIR%\\deployment_tools\\tools\\model_downloader\\converter.py\"  --name googlenet-v1 --precisions FP16\n",
    "!dir public\\googlenet-v1\\FP16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "The Python inferencing code starts from here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize application for OpenVINO\n",
    "This part is identical to the program in the previous image classification exercise. \n",
    "1. Import required Python modules\n",
    "2. Load class label text file\n",
    "3. Create an inference engine core object\n",
    "4. Load IR model to memory\n",
    "5. Obtain information of input and output blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from openvino.inference_engine import IENetwork, IECore\n",
    "\n",
    "label = open('synset_words.txt').readlines()\n",
    "\n",
    "# Create an Inference Engine core object\n",
    "ie = IECore()\n",
    "\n",
    "# Read an IR model data to memory\n",
    "model = './public/googlenet-v1/FP16/googlenet-v1'\n",
    "net = IENetwork(model=model+'.xml', weights=model+'.bin')\n",
    "\n",
    "# Obtain the name of the input and output blob, and input blob shape\n",
    "input_blob_name  = list(net.inputs.keys())[0]\n",
    "output_blob_name = list(net.outputs.keys())[0]\n",
    "batch,channel,height,width = net.inputs[input_blob_name].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) `Query` API\n",
    "Inference engine has Query API and you can obtain some information from IE plugins with query keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 1)\n",
      "(1, 8)\n"
     ]
    }
   ],
   "source": [
    "print(ie.get_metric('CPU', 'RANGE_FOR_ASYNC_INFER_REQUESTS'))\n",
    "print(ie.get_metric('CPU', 'RANGE_FOR_STREAMS'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plugin configuration\n",
    "You can set special parameters to IE plugins using `set_config()` API.  \n",
    "The **inferencing performance will be boosted** by configuring parameters such as `CPU_THREAD_NUM`, `CPU_BIND_THREAD`, `CPU_THROUGHPUT_STREAMS` properly.  \n",
    "The other plugins or devices has its own special paramter keys. Please refer to the OpenVINO technical document library for details.\n",
    "https://docs.openvinotoolkit.org/latest/_docs_IE_DG_supported_plugins_Supported_Devices.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ie.set_config({'CPU_THREADS_NUM'       : '0'   }, 'CPU')  # default = 0\n",
    "ie.set_config({'CPU_BIND_THREAD'       : 'YES' }, 'CPU')  # default = YES\n",
    "ie.set_config({'CPU_THROUGHPUT_STREAMS': '1'   }, 'CPU')  # default = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading model data to the IE core object\n",
    "Load the model data to the IE core object.  \n",
    "You can specify how many infer request objects to be generated with the `num_requests` parameter.  \n",
    "You can submit the same number of infer requests to Inference Engine as the number of infer request objects. (Meaning, one infer request can be sent per infer request object)\n",
    "\n",
    "Here, we create 4 infer request objects, you can run 4 simultaneous inferencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_net = ie.load_network(network=net, device_name='CPU', num_requests=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting callback function to the infer_request object\n",
    "Set a callback function to the `infer_request` object. You can use noname function (lambd espression) as well.  \n",
    "\n",
    "In this case, callback does nothing but counting completed infer requests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_infer=0\n",
    "\n",
    "def callback(status_code, output):\n",
    "    global total_infer\n",
    "    total_infer  += 1\n",
    "\n",
    "for req in exec_net.requests:\n",
    "    req.set_completion_callback(callback, req.outputs[output_blob_name][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and manipulate input image\n",
    "Read the input image file and resize and transform it to fit it for input blob of the DL model using OpenCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('car.png')\n",
    "img = cv2.resize(img, (width,height))\n",
    "img = img.transpose((2, 0, 1))\n",
    "img = img.reshape((1, channel, height, width))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running inference  \n",
    "- Run inference (400 inferences in asynchronous and 4 inferences at a time)\n",
    "- Wait for the completion of all inference tasks\n",
    "- Display performance data and inference result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_infer=4 time=7.766sec fps=52.021559075395274\n",
      "\n",
      "infer_request  0\n",
      "480 0.4227575 car wheel\n",
      "818 0.40495834 sports car, sport car\n",
      "512 0.09181638 convertible\n",
      "437 0.023778602 beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon\n",
      "752 0.018072149 racer, race car, racing car\n",
      "infer_request  1\n",
      "480 0.4227575 car wheel\n",
      "818 0.40495834 sports car, sport car\n",
      "512 0.09181638 convertible\n",
      "437 0.023778602 beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon\n",
      "752 0.018072149 racer, race car, racing car\n",
      "infer_request  2\n",
      "480 0.4227575 car wheel\n",
      "818 0.40495834 sports car, sport car\n",
      "512 0.09181638 convertible\n",
      "437 0.023778602 beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon\n",
      "752 0.018072149 racer, race car, racing car\n",
      "infer_request  3\n",
      "480 0.4227575 car wheel\n",
      "818 0.40495834 sports car, sport car\n",
      "512 0.09181638 convertible\n",
      "437 0.023778602 beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon\n",
      "752 0.018072149 racer, race car, racing car\n"
     ]
    }
   ],
   "source": [
    "# (Workaround for a bug in Python API. Run dummy inferencing on all infer_request objects)\n",
    "for req in exec_net.requests:\n",
    "    req.async_infer(inputs={input_blob_name: img})\n",
    "\n",
    "infer_slot = 0\n",
    "total_infer= 0\n",
    "max_infer = len(exec_net.requests)\n",
    "\n",
    "start=time.time()\n",
    "\n",
    "# Run inference 400 times\n",
    "while total_infer<400:\n",
    "    req = exec_net.requests[infer_slot]\n",
    "    status = req.wait(0)\n",
    "    if status == 0 or status==-11:   # Send infer request to IE when infer_request status is 0(OK) or -11(INFER_NOT_STARTED)\n",
    "        res = req.async_infer(inputs={input_blob_name: img})\n",
    "    infer_slot = (infer_slot+1) % max_infer\n",
    "\n",
    "# Wait until all inference requests are completed\n",
    "for req in exec_net.requests:\n",
    "    while req.wait()!=0: pass\n",
    "\n",
    "# Display performance data\n",
    "total=time.time()-start\n",
    "print('max_infer={} time={:.4}sec fps={}\\n'.format(max_infer, total, total_infer/total))\n",
    "\n",
    "# Display inference result\n",
    "for i, req in enumerate(exec_net.requests):\n",
    "    output = req.outputs[output_blob_name][0]\n",
    "    idx = np.argsort(output)[::-1]\n",
    "    print('infer_request ', i)\n",
    "    for i in range(5):\n",
    "        print(idx[i]+1, output[idx[i]], label[idx[i]][:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Now, you have learnt the basic technique of developing an efficient and high performance OpenVINO program using asynchronous and simultaneous inferencing.  \n",
    "The points in this exercise are:\n",
    "- Using asynchronous inference\n",
    "- Send appropriate number infer requests to the processor to keep saturate (busy) the processor\n",
    "\n",
    "This time, we used the default value for `CPU_THREAD_NUM`, `CPU_BIND_THREAD`, and `CPU_THROUGHPUT_STREAMS`.  \n",
    "The inferencing performance **could be more than double** (on DevCloud development server) if you set optimal value for those parameters and tweak `num_requests`.\n",
    "\n",
    "Try find the best configuration by modifying those parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~cpp\n",
    "enum StatusCode : int {\n",
    "  OK = 0, GENERAL_ERROR = -1, NOT_IMPLEMENTED = -2, NETWORK_NOT_LOADED = -3,\n",
    "  PARAMETER_MISMATCH = -4, NOT_FOUND = -5, OUT_OF_BOUNDS = -6, UNEXPECTED = -7,\n",
    "  REQUEST_BUSY = -8, RESULT_NOT_READY = -9, NOT_ALLOCATED = -10, INFER_NOT_STARTED = -11,\n",
    "  NETWORK_NOT_READ = -12\n",
    "}\n",
    "~~~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
