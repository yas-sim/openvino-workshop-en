{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technique for high performance inference program - asynchronous and simultaneous inferencing\n",
    "You will learn the basic technique to develop an efficient and high performance OpenVINO application using asynchronous and simultaneous inferencing.   \n",
    "We'll continue to use a simple image classification program as an example to make things as simple as possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check hardware configration of your system (DevCloud development server, in this case)\n",
    "Before start, let's check how many CPU cores does the system has for the optimization work later.  \n",
    "Install `psutil` Python module to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linux\n",
    "!pip3 install psutil\n",
    "import psutil\n",
    "print('# of CPU cores = {}C/{}T'.format(psutil.cpu_count(logical=False), psutil.cpu_count(logical=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows\n",
    "!pip install psutil\n",
    "import psutil\n",
    "print('# of CPU cores = {}C/{}T'.format(psutil.cpu_count(logical=False), psutil.cpu_count(logical=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing an input image and label text data files\n",
    "Next, let's prepare imput image file and class label text file. Those files are in the OpenVINO install directory. We'll simply copy them to the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linux\n",
    "!cp $INTEL_OPENVINO_DIR/deployment_tools/demo/car.png .\n",
    "!cp $INTEL_OPENVINO_DIR/deployment_tools/demo/squeezenet1.1.labels synset_words.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows\n",
    "!copy \"%INTEL_OPENVINO_DIR%\\deployment_tools\\demo\\car.png\" .\n",
    "!copy \"%INTEL_OPENVINO_DIR%\\deployment_tools\\demo\\squeezenet1.1.labels\" synset_words.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing a DL model for inferencing\n",
    "Download a DL model for image classification using `Model downloader` and convert it into OpenVINO IR model with `Model converter`.  \n",
    "We'll use `googlenet-v1` model for this practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linux\n",
    "!python3 $INTEL_OPENVINO_DIR/deployment_tools/open_model_zoo/tools/model_downloader/downloader.py --name googlenet-v3\n",
    "!python3 $INTEL_OPENVINO_DIR/deployment_tools/open_model_zoo/tools/model_downloader/converter.py  --name googlenet-v3 --precisions FP16\n",
    "!ls public/googlenet-v3/FP16 -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows\n",
    "!python \"%INTEL_OPENVINO_DIR%\\deployment_tools\\open_model_zoo\\tools\\downloader\\downloader.py\" --name googlenet-v3\n",
    "!python \"%INTEL_OPENVINO_DIR%\\deployment_tools\\open_model_zoo\\tools\\downloader\\converter.py\"  --name googlenet-v3 --precisions FP16\n",
    "!dir public\\googlenet-v3\\FP16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "The Python inferencing code starts from here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize application for OpenVINO\n",
    "This part is identical to the program in the previous image classification exercise. \n",
    "1. Import required Python modules\n",
    "2. Load class label text file\n",
    "3. Create an inference engine core object\n",
    "4. Load IR model to memory\n",
    "5. Obtain information of input and output blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from openvino.inference_engine import IECore\n",
    "\n",
    "label = [ s.replace('\\n', '') for s in open('synset_words.txt').readlines() ]\n",
    "\n",
    "# Create an Inference Engine core object\n",
    "ie = IECore()\n",
    "\n",
    "# Read an IR model data to memory\n",
    "model = './public/googlenet-v3/FP16/googlenet-v3'\n",
    "net = ie.read_network(model=model+'.xml', weights=model+'.bin')\n",
    "\n",
    "# Obtain the name of the input and output blob, and input blob shape\n",
    "input_blob_name  = list(net.input_info.keys())[0]\n",
    "output_blob_name = list(net.outputs.keys())[0]\n",
    "batch,channel,height,width = net.input_info[input_blob_name].tensor_desc.dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) `Query` API\n",
    "Inference engine has Query API and you can obtain some information from IE plugins with query keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ie.get_metric('CPU', 'RANGE_FOR_ASYNC_INFER_REQUESTS'))\n",
    "print(ie.get_metric('CPU', 'RANGE_FOR_STREAMS'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plugin configuration\n",
    "You can set special parameters to IE plugins using `set_config()` API.  \n",
    "The **inferencing performance will be boosted** by configuring parameters such as `CPU_THREAD_NUM`, `CPU_BIND_THREAD`, `CPU_THROUGHPUT_STREAMS` properly.  \n",
    "The other plugins or devices has its own special paramter keys. Please refer to the OpenVINO technical document library for details.\n",
    "https://docs.openvinotoolkit.org/latest/_docs_IE_DG_supported_plugins_Supported_Devices.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ie.set_config({'CPU_THREADS_NUM'       : '0'   }, 'CPU')  # default = 0\n",
    "ie.set_config({'CPU_BIND_THREAD'       : 'YES' }, 'CPU')  # default = YES\n",
    "ie.set_config({'CPU_THROUGHPUT_STREAMS': '1'   }, 'CPU')  # default = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading model data to the IE core object\n",
    "Load the model data to the IE core object.  \n",
    "You can specify how many infer request objects to be generated with the `num_requests` parameter.  \n",
    "You can submit the same number of infer requests to Inference Engine as the number of infer request objects. (Meaning, one infer request can be sent per infer request object)\n",
    "\n",
    "Here, we create 4 infer request objects, you can run 4 simultaneous inferencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_net = ie.load_network(network=net, device_name='CPU', num_requests=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting callback function to the infer_request object\n",
    "Set a callback function to the `infer_request` object. You can use noname function (lambd espression) as well.  \n",
    "\n",
    "In this case, callback does nothing but counting completed infer requests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_infer=0\n",
    "\n",
    "def callback(status_code, output):\n",
    "    global total_infer\n",
    "    total_infer  += 1\n",
    "\n",
    "for req in exec_net.requests:\n",
    "    req.set_completion_callback(callback, req.output_blobs[output_blob_name].buffer[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and manipulate input image\n",
    "Read the input image file and resize and transform it to fit it for input blob of the DL model using OpenCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('car.png')\n",
    "img = cv2.resize(img, (width,height))\n",
    "img = img.transpose((2, 0, 1))\n",
    "img = img.reshape((1, channel, height, width))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running inference  \n",
    "- Run inference (400 inferences in asynchronous and 4 inferences at a time)\n",
    "- Wait for the completion of all inference tasks\n",
    "- Display performance data and inference result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Workaround for a bug in Python API. Run dummy inferencing on all infer_request objects)\n",
    "for req in exec_net.requests:\n",
    "    req.async_infer(inputs={input_blob_name: img})\n",
    "\n",
    "infer_slot = 0\n",
    "total_infer= 0\n",
    "max_infer = len(exec_net.requests)\n",
    "\n",
    "start=time.time()\n",
    "\n",
    "# Run inference 400 times\n",
    "while total_infer<400:\n",
    "    req = exec_net.requests[infer_slot]\n",
    "    status = req.wait(0)\n",
    "    if status == 0 or status==-11:   # Send infer request to IE when infer_request status is 0(OK) or -11(INFER_NOT_STARTED)\n",
    "        res = req.async_infer(inputs={input_blob_name: img})\n",
    "    infer_slot = (infer_slot+1) % max_infer\n",
    "\n",
    "# Wait until all inference requests are completed\n",
    "for req in exec_net.requests:\n",
    "    while req.wait()!=0: pass\n",
    "\n",
    "# Display performance data\n",
    "total=time.time()-start\n",
    "print('max_infer={} time={:.4}sec fps={}\\n'.format(max_infer, total, total_infer/total))\n",
    "\n",
    "# Display inference result\n",
    "for i, req in enumerate(exec_net.requests):\n",
    "    output = req.output_blobs[output_blob_name].buffer[0]\n",
    "    idx = np.argsort(output)[::-1]\n",
    "    print('infer_request ', i)\n",
    "    for i in range(5):\n",
    "        print(idx[i], output[idx[i]], label[idx[i]-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Now, you have learnt the basic technique of developing an efficient and high performance OpenVINO program using asynchronous and simultaneous inferencing.  \n",
    "The points in this exercise are:\n",
    "- Using asynchronous inference\n",
    "- Send appropriate number infer requests to the processor to keep saturate (busy) the processor\n",
    "\n",
    "This time, we used the default value for `CPU_THREAD_NUM`, `CPU_BIND_THREAD`, and `CPU_THROUGHPUT_STREAMS`.  \n",
    "The inferencing performance **could be more than double** (on DevCloud development server) if you set optimal value for those parameters and tweak `num_requests`.\n",
    "\n",
    "Try find the best configuration by modifying those parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~cpp\n",
    "enum StatusCode : int {\n",
    "  OK = 0, GENERAL_ERROR = -1, NOT_IMPLEMENTED = -2, NETWORK_NOT_LOADED = -3,\n",
    "  PARAMETER_MISMATCH = -4, NOT_FOUND = -5, OUT_OF_BOUNDS = -6, UNEXPECTED = -7,\n",
    "  REQUEST_BUSY = -8, RESULT_NOT_READY = -9, NOT_ALLOCATED = -10, INFER_NOT_STARTED = -11,\n",
    "  NETWORK_NOT_READ = -12\n",
    "}\n",
    "~~~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "interpreter": {
   "hash": "c9449627750e98e9b1ad30fb225936bb035c6d0e9c862047946b304a9e5cb973"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}