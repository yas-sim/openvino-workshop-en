{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic of asynchronous inferencing\n",
    "It is very important to utilize asynchronous inferencing capability of OpenVINO to make your application efficient and high performance.  \n",
    "The `infer()` API used in the synchronous inferencing is a blocking function and the program will be kept waiting until inferencing is completed. So, the other task can't be performed in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing an input image and label text data files\n",
    "First, let's prepare imput image file and class label text file. Those files are in the OpenVINO install directory. We'll simply copy them to the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp $INTEL_OPENVINO_DIR/deployment_tools/demo/car.png .\n",
    "!cp $INTEL_OPENVINO_DIR/deployment_tools/demo/squeezenet1.1.labels synset_words.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the image file and check the picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('car.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing a DL model for inferencing\n",
    "Download a DL model for image classification using `Model downloader` and convert it into OpenVINO IR model with `Model converter`.  \n",
    "We'll use `googlenet-v1` model for this practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 $INTEL_OPENVINO_DIR/deployment_tools/tools/model_downloader/downloader.py --name googlenet-v1\n",
    "!python3 $INTEL_OPENVINO_DIR/deployment_tools/tools/model_downloader/converter.py  --name googlenet-v1 --precisions FP16\n",
    "!ls public/googlenet-v1/FP16 -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "The Python inferencing code starts from here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize application for OpenVINO\n",
    "This part is identical to the program in the previous image classification exercise. \n",
    "1. Import required Python modules\n",
    "2. Load class label text file\n",
    "3. Create an inference engine core object\n",
    "4. Load IR model to memory\n",
    "5. Obtain information of input and output blob\n",
    "6. Load model data to inference engine core object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from openvino.inference_engine import IENetwork, IECore\n",
    "\n",
    "label = open('synset_words.txt').readlines()\n",
    "\n",
    "# Create an Inference Engine core object\n",
    "ie = IECore()\n",
    "\n",
    "# Read an IR model data to memory\n",
    "model = './public/googlenet-v1/FP16/googlenet-v1'\n",
    "net = IENetwork(model=model+'.xml', weights=model+'.bin')\n",
    "\n",
    "# Obtain the name of the input and output blob, and input blob shape\n",
    "input_blob_name  = list(net.inputs.keys())[0]\n",
    "output_blob_name = list(net.outputs.keys())[0]\n",
    "batch,channel,height,width = net.inputs[input_blob_name].shape\n",
    "\n",
    "exec_net = ie.load_network(network=net, device_name='CPU', num_requests=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting callback function to the infer_request object\n",
    "Set a callback function to the `infer_request` object. You can use noname function (lambd espression) as well.  \n",
    "The second parameter in `set_completion_callback()` API is a user data and will be passed to the callback function. In this code, `exec_net.requests[req_id]` will be passed to the callback function to display the result.  \n",
    "The callback function displays the result of inferencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback(status_code, infer_request):\n",
    "    output = infer_request.outputs[output_blob_name][0]\n",
    "    idx = np.argsort(output)[::-1]\n",
    "    for i in range(5):\n",
    "        print(idx[i]+1, output[idx[i]], label[idx[i]][:-1])\n",
    "\n",
    "req_id=0\n",
    "exec_net.requests[req_id].set_completion_callback(callback, exec_net.requests[req_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and manipulate input image\n",
    "Read the input image file and resize and transform it to fit it for input blob of the DL model using OpenCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('car.png')\n",
    "img = cv2.resize(img, (width,height))\n",
    "img = img.transpose((2, 0, 1))\n",
    "img = img.reshape((1, channel, height, width))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running inference  \n",
    "The `start_async()` API will kick a thread and start inferencing in parallel, and the control will be returned to your program immediately.  So, the number displaying code will run while inferencing is ongoing. The inferencing will finish in the middle of number display and the result will be displayed in between of numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = exec_net.start_async(0, inputs={input_blob_name: img})\n",
    "\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    time.sleep(0.1)    # wait for 0.2 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Now, you have learnt how to use aynchronous inference API of OpenVINO.\n",
    "\n",
    "You can use the hardware resources in very efficient manner and could achieve high performance.  \n",
    "Especially it is important when you offload your inferencing task from CPU to other devices such as GPU, MYRIAD and FPGA. If you use a synchronous API to inference, the CPU will just waste processing capability and wait for completion of the inferencing task performed in the other devices.  \n",
    "\n",
    "Get familiar with the synchronous inferencing API and develop an efficient and high performance deep learning application with OpenVINO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next => Technique for high performance inference program - asynchronous and simultaneous inferencing - [classification-async-multi.ipynb](./classification-async-multi.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
